{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uwI9bZ9FSwVN"
      },
      "outputs": [],
      "source": [
        "from gym import Env\n",
        "from gym.spaces import Discrete, Box, Dict\n",
        "from collections import OrderedDict\n",
        "from IPython import display\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "import pygame\n",
        "import torch\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "rng = np.random.default_rng()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gswb4YYkHiw5",
        "outputId": "55d05b51-b064-4573-9610-6fa51e63eb24"
      },
      "outputs": [],
      "source": [
        "# parameters\n",
        "car_width = 150\n",
        "car_length = 400\n",
        "max_loop = 5000\n",
        "fps = 60\n",
        "car_hypotenuse = math.sqrt((car_length/2)**2 + (car_width/2)**2)\n",
        "hypotenuse_angle = math.acos(car_width/(2*car_hypotenuse))\n",
        "car_wheelbase=250\n",
        "parking_width = 240\n",
        "parking_length = 450\n",
        "road_width = 300\n",
        "area_width = (2 * road_width) + (2 * parking_length) #1300\n",
        "area_length = 8 * parking_width #1920\n",
        "timestep=1\n",
        "max_steering=30\n",
        "max_velocity=30 #in m/s\n",
        "speed_limit=10\n",
        "max_acceleration=3.4 #around 12kmh/s\n",
        "max_decceleration=6\n",
        "natural_decceleration=1\n",
        "\n",
        "car_image = pygame.image.load('car_sprite.png')\n",
        "car_image = pygame.transform.scale(car_image, (car_width, car_length))\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "# device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class CarParking(Env):\n",
        "  def __init__(self):\n",
        "    # 1=right, 2=left, 3=stay\n",
        "    # 1=accelerate, 2=deccelerate, 3=same speed and direction\n",
        "    self.action_space=Box(low=np.array([1,1]),high=np.array([3,3]),dtype=int)\n",
        "    self.observation_space=Dict({\n",
        "        'velocity': Box(low=np.array([0]),high=np.array([150]))\n",
        "        ,'acceleration': Box(low=np.array([0]),high=np.array([150]))\n",
        "        ,'angle': Box(low=np.array([0]),high=np.array([359])) #angle of car\n",
        "        ,'pos': Box(low=np.array([0, 0]), high=np.array([area_length-1, area_width-1])) #calculate 4 corners of car using trigonometry\n",
        "        ,'steering': Discrete(61,start=-30)#angle of steering wheel\n",
        "        ,'wheels_inside': Discrete(5,start=0)\n",
        "        ,'distances': Box(low=np.array([0,0,0,0]),high=np.array([area_width]))\n",
        "        ,'y_dist': Box(low=np.array([0]),high=np.array([area_length]))\n",
        "        })\n",
        "\n",
        "    self.screen = None #pygame.display.set_mode((area_length, area_width))\n",
        "    self.clock = None #pygame.time.Clock()\n",
        "    self.max_iterations = 1000\n",
        "    self.set_vars()\n",
        "\n",
        "\n",
        "  def check_if_inside(self):\n",
        "    goal = True\n",
        "    wheels_inside = 0\n",
        "    reward = 0\n",
        "    for wheel in ['lb','rb','lf','rf']:\n",
        "      if self.wheels[wheel][0] < 0 or self.wheels[wheel][0] >= area_length or self.wheels[wheel][1] < 0 or self.wheels[wheel][1] >= area_width:\n",
        "        return True,-10000\n",
        "      if self.wheels[wheel][0] < self.goal[0][0] or self.wheels[wheel][0] > self.goal[1][0] or self.wheels[wheel][1] < self.goal[0][1] or self.wheels[wheel][1] > self.goal[2][1]:\n",
        "        goal = False\n",
        "      else:\n",
        "        wheels_inside += 1\n",
        "        reward += 1\n",
        "    self.state['wheels_inside'] = wheels_inside\n",
        "    if goal:\n",
        "      return True, 5000\n",
        "    return False, reward\n",
        "\n",
        "  def close(self):\n",
        "    if self.screen:\n",
        "\n",
        "      pygame.quit()\n",
        "      self.screen=None\n",
        "      self.clock=None\n",
        "\n",
        "  def process_new_position(self, reward):\n",
        "      self.state['velocity'] += self.state['acceleration']\n",
        "      if abs(self.state['velocity']) > speed_limit:\n",
        "        reward -= 1\n",
        "      self.state['velocity'] = max(-max_velocity, min(self.state['velocity'], max_velocity))\n",
        "\n",
        "      if self.state['steering']:\n",
        "        turning_radius = car_wheelbase / math.sin(math.radians(self.state['steering']))\n",
        "        angular_velocity = self.state['velocity'] / turning_radius\n",
        "      else:\n",
        "        angular_velocity = 0\n",
        "\n",
        "      self.state['pos'] += np.array([self.state['velocity'] * math.sin(math.radians(self.state['angle']))*timestep,-self.state['velocity'] * math.cos(math.radians(self.state['angle']))*timestep])\n",
        "      self.state['angle'] = (self.state['angle'] + math.degrees(angular_velocity) * timestep)%360\n",
        "      if self.state['angle'] < 0:\n",
        "        self.state['angle']+= 360\n",
        "      \n",
        "\n",
        "  def calculate_four_corners(self):\n",
        "    rb = self.state['pos'] + np.array([car_hypotenuse*math.cos(math.radians(self.state['angle'])+hypotenuse_angle),\n",
        "                                                  car_hypotenuse*math.sin(math.radians(self.state['angle'])+hypotenuse_angle)])\n",
        "    lb = self.state['pos'] + np.array([car_hypotenuse*math.cos(math.radians(self.state['angle'])+math.pi-hypotenuse_angle),\n",
        "                                                  car_hypotenuse*math.sin(math.radians(self.state['angle'])+math.pi-hypotenuse_angle)])\n",
        "    rf = self.state['pos'] + np.array([car_hypotenuse*math.cos(math.radians(self.state['angle'])+2*math.pi-hypotenuse_angle),\n",
        "                                                  car_hypotenuse*math.sin(math.radians(self.state['angle'])+2*math.pi-hypotenuse_angle)])\n",
        "    lf = self.state['pos'] + np.array([car_hypotenuse*math.cos(math.radians(self.state['angle'])+math.pi+hypotenuse_angle),\n",
        "                                                  car_hypotenuse*math.sin(math.radians(self.state['angle'])+math.pi+hypotenuse_angle)])\n",
        "\n",
        "    return rf, lf, rb, lb\n",
        "  \n",
        "  def distance_to_parking(self, car_pos, park_pos):\n",
        "    return np.linalg.norm([car_pos[0]-park_pos[0], car_pos[1]-park_pos[1]])\n",
        "\n",
        "  def step(self, action):\n",
        "    if not self.clock:\n",
        "      self.clock = pygame.time.Clock()\n",
        "    reward = 0\n",
        "    accelerate = True\n",
        "    # timestep = self.clock.get_time() / 1000\n",
        "    self.current_iterations += 1\n",
        "    action = np.array([int((action-1)/3)+1,((action-1)%3)+1]) \n",
        "    if action[0]==1:\n",
        "      \n",
        "      self.state['steering']+=max_steering*timestep\n",
        "    elif action[0]==2:\n",
        "      self.state['steering']-=max_steering*timestep\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "    if self.state['steering'] != max(-max_steering, min(self.state['steering'], max_steering)):\n",
        "      reward -= 1\n",
        "      self.state['steering'] = max(-max_steering, min(self.state['steering'], max_steering))\n",
        "\n",
        "    if action[1]==1:\n",
        "      if self.state['velocity']>=0:\n",
        "        \n",
        "        self.state['acceleration'] += random.uniform(max_acceleration-0.05,max_acceleration+0.05) * timestep * 0.005\n",
        "      else:\n",
        "        accelerate=False\n",
        "        if self.state['acceleration'] < 0:\n",
        "          self.state['acceleration'] = 0\n",
        "        self.state['acceleration'] += random.uniform(max_decceleration-0.1,max_decceleration+0.1) * timestep * 0.005\n",
        "        if abs(self.state['velocity']) < abs(self.state['acceleration']):\n",
        "          self.state['acceleration'] = -self.state['velocity']\n",
        "    elif action[1]==2:\n",
        "      if self.state['velocity']<=0:\n",
        "        self.state['acceleration'] -= random.uniform(max_acceleration-0.05,max_acceleration+0.05) * timestep * 0.005\n",
        "      else:\n",
        "        accelerate=False\n",
        "        if self.state['acceleration'] > 0:\n",
        "          self.state['acceleration'] = 0\n",
        "        self.state['acceleration'] -= random.uniform(max_decceleration-0.1,max_decceleration+0.1) * timestep * 0.005\n",
        "        if abs(self.state['velocity']) < abs(self.state['acceleration']):\n",
        "          self.state['acceleration'] = -self.state['velocity']\n",
        "    else: #action[1]==3:\n",
        "      self.state['acceleration']=0\n",
        "      # accelerate=False\n",
        "      # if abs(self.state['velocity']) > timestep * natural_decceleration:\n",
        "      #     self.state['acceleration']= -math.copysign(natural_decceleration, self.state['acceleration'])\n",
        "      # else:\n",
        "      #     self.state['acceleration'] = -self.state['velocity']\n",
        "\n",
        "    if accelerate:\n",
        "      if self.state['acceleration'] != max(-max_acceleration, min(self.state['acceleration'], max_acceleration)):\n",
        "        reward-=1 #unecessary action\n",
        "      self.state['acceleration'] = max(-max_acceleration, min(self.state['acceleration'], max_acceleration))\n",
        "    else:\n",
        "      if self.state['acceleration'] != max(-max_decceleration, min(self.state['acceleration'], max_decceleration)):\n",
        "        reward -= 1\n",
        "      self.state['acceleration'] != max(-max_decceleration, min(self.state['acceleration'], max_decceleration))\n",
        "\n",
        "    self.process_new_position(reward)\n",
        "    self.wheels['rf'], self.wheels['lf'], self.wheels['rb'], self.wheels['lb'] = self.calculate_four_corners()\n",
        "    self.state['distances'] = self.calculate_distances()\n",
        "    self.state['angle_from_parking'] = self.calculate_angle_from_parking()\n",
        "    # calculate reward\n",
        "    reward -= 1\n",
        "    reward += 500/self.distance_to_parking(self.state['pos'], np.array(self.goal_center))\n",
        "\n",
        "    # check if done\n",
        "    done, reward_gain = self.check_if_inside()\n",
        "    if done and reward_gain < -1000:\n",
        "      reward = reward_gain\n",
        "    else:\n",
        "      reward += reward_gain\n",
        "\n",
        "\n",
        "    \n",
        "    info = {}\n",
        "    coords = None\n",
        "    self.clock.tick(60)\n",
        "    if self.current_iterations == self.max_iterations:\n",
        "      done = True\n",
        "\n",
        "    return self.state, reward, done, info, coords\n",
        "  \n",
        "  def render(self):\n",
        "    if not self.screen:\n",
        "      pygame.init()\n",
        "      pygame.display.init()\n",
        "      self.screen = pygame.display.set_mode((area_length, area_width), pygame.FULLSCREEN | pygame.SCALED)\n",
        "      \n",
        "    self.screen.fill((255, 255, 255))\n",
        "    if self.goal_number < 8:\n",
        "      goal = pygame.Rect(self.goal[0][0], self.goal[0][1], parking_width, parking_length)\n",
        "    else:\n",
        "      goal = pygame.Rect(self.goal[2][0], self.goal[2][1], parking_width, parking_length)\n",
        "    pygame.draw.rect(self.screen, (0,0,255), goal)\n",
        "    \n",
        "    for i in range(7):\n",
        "      rectangle_top = pygame.Rect(parking_width*(i+1)-1, 0, 3, parking_length)\n",
        "      rectangle_bottom = pygame.Rect(parking_width*(i+1)-1, parking_length + 2 * road_width-1, 3, parking_length)\n",
        "      pygame.draw.rect(self.screen, (0,0,0), rectangle_top)\n",
        "      pygame.draw.rect(self.screen, (0,0,0), rectangle_bottom)\n",
        "\n",
        "    \n",
        "    rotated = pygame.transform.rotate(car_image, -self.state['angle'])\n",
        "    rotated_rect=rotated.get_rect()\n",
        "    rotated_rect.center = self.state['pos']\n",
        "    self.screen.blit(rotated, rotated_rect)\n",
        "    pygame.draw.circle(self.screen,(0,255,0),self.state['pos'],5)\n",
        "    pygame.draw.circle(self.screen,(0,255,255),self.goal_center,10)\n",
        "    for part in ['rf','lf','rb','lb']:\n",
        "      pygame.draw.circle(self.screen, (255,0,0), self.wheels[part], 5)\n",
        "    pygame.display.update()\n",
        "\n",
        "  def set_vars(self):\n",
        "    x_noise = random.uniform(-(road_width-car_width)/2,(road_width-car_width)/2)\n",
        "    y_noise = random.uniform(0,50)\n",
        "    self.state = OrderedDict({\n",
        "        'velocity': 0\n",
        "        ,'acceleration': 0\n",
        "        ,'angle': 90\n",
        "        ,'pos': np.array([(car_length/2),parking_length+(road_width/2)-1])\n",
        "        ,'steering': 0\n",
        "        ,'wheels_inside': 0}) \n",
        "    self.goal_number = random.randint(0,15)\n",
        "    self.wheels = {}\n",
        "    self.wheels['rf'], self.wheels['lf'], self.wheels['rb'], self.wheels['lb'] = self.calculate_four_corners()\n",
        "    self.current_iterations = 0\n",
        "    if self.goal_number < 8:\n",
        "      self.goal = np.array([[(self.goal_number*parking_width),0], #top left\n",
        "                            [((self.goal_number+1)*parking_width)-1,0], #top right\n",
        "                            [(self.goal_number*parking_width),parking_length-1], #inside left\n",
        "                            [((self.goal_number+1)*parking_width)-1,parking_length-1]]) #inside right\n",
        "    else:\n",
        "      self.goal = np.array([[((self.goal_number-8)*parking_width),area_width-1], #bottom left\n",
        "                            [((self.goal_number-7)*parking_width)-1,area_width-1], #bottom right\n",
        "                            [((self.goal_number-8)*parking_width),parking_length+(2*road_width)], #inside left\n",
        "                            [((self.goal_number-7)*parking_width)-1,parking_length+(2*road_width)]]) #inside right\n",
        "    # get distance of all wheels to the two outside parking lines\n",
        "    self.state['distances'] = self.calculate_distances()\n",
        "    #get angle\n",
        "    self.goal_center = np.array(np.mean(self.goal, axis=0))\n",
        "    self.state['angle_from_parking'] = self.calculate_angle_from_parking()\n",
        "    self.normalisation_table = {\n",
        "      'velocity': [max_velocity,max_velocity*2], #plus by min, divide by range\n",
        "      'acceleration': [max_acceleration, max_acceleration*2],\n",
        "      'angle': [0, 360],\n",
        "      'pos_x': [0, area_length],\n",
        "      'pos_y': [0, area_width],\n",
        "      'steering': [max_steering, max_steering * 2],\n",
        "      'wheels_inside': [0, 4],\n",
        "      'distances': [0, np.linalg.norm([area_length, area_width])],\n",
        "      'angle_from_parking': [180, 360]\n",
        "    }\n",
        "\n",
        "  def calculate_distances(self):\n",
        "    distances = np.zeros(4)\n",
        "    for i, wheel in enumerate(['rf','lf','rb','lb']):\n",
        "      distances[i] = np.linalg.norm(self.wheels[wheel]-np.mean(self.goal[:2],axis=0))\n",
        "    return np.array(distances)\n",
        "\n",
        "  def calculate_angle_from_parking(self):\n",
        "    return math.degrees(np.arctan2(self.goal_center[1] - self.state['pos'][1], abs(self.goal_center[0] - self.state['pos'][0])))\n",
        "\n",
        "\n",
        "  def reset(self, seed=None, options=None):\n",
        "    super().reset(seed=seed)\n",
        "    self.set_vars()\n",
        "    return self.state\n",
        "  \n",
        "  def convert_actions(self, action):\n",
        "    return action[1] + ((action[0]-1)*3)\n",
        "  \n",
        "  def deconstruct_array(self,arr):\n",
        "    llist = []\n",
        "    for key in ['velocity','acceleration','angle','pos','steering','wheels_inside','distances','angle_from_parking']:\n",
        "      if key =='pos':\n",
        "        new_arr = arr[key].flatten()\n",
        "        new_arr[0] = (new_arr[0]+self.normalisation_table['pos_x'][0])/self.normalisation_table['pos_x'][1]\n",
        "        new_arr[1] = (new_arr[1]+self.normalisation_table['pos_y'][0])/self.normalisation_table['pos_y'][1]\n",
        "        llist.extend(new_arr.tolist())\n",
        "      elif key == 'distances':\n",
        "        new_arr =(arr[key].flatten()+self.normalisation_table[key][0])/self.normalisation_table[key][1]\n",
        "        llist.extend(new_arr.tolist())\n",
        "      else:\n",
        "        llist.append((arr[key]+self.normalisation_table[key][0])/self.normalisation_table[key][1])\n",
        "        \n",
        "    return np.array(llist)\n",
        "  \n",
        "env = CarParking()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "#dqn and agent\n",
        "\n",
        "# Q-network for approximating action-value function\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, learning_rate):\n",
        "        super().__init__()\n",
        "        # create network layers\n",
        "        layers = nn.ModuleList()\n",
        "        \n",
        "        # input layer\n",
        "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
        "        layers.append(nn.ReLU())\n",
        "        \n",
        "        # hidden layers\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
        "            layers.append(nn.ReLU())\n",
        "        \n",
        "        # output layer\n",
        "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
        "    \n",
        "        # combine layers into feed-forward network\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        \n",
        "        # select loss function and optimizer\n",
        "        # note: original paper uses modified MSE loss and RMSprop\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.optimizer = torch.optim.AdamW(self.net.parameters(), lr=learning_rate)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # return output of Q-network for the input x\n",
        "        return self.net(x)\n",
        "    \n",
        "    def update(self, inputs, targets):\n",
        "        # update network weights for a minibatch of inputs and targets:\n",
        "        self.optimizer.zero_grad()\n",
        "        outputs = self.net(inputs)\n",
        "        loss = self.criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "    \n",
        "    def copy_from(self, qnetwork):\n",
        "        # copy weights from another Q-network\n",
        "        self.net.load_state_dict(qnetwork.net.state_dict())\n",
        "    \n",
        "\n",
        "# Deep Q-network (DQN)\n",
        "class AgentDQN():\n",
        "    def __init__(self, env, gamma,\n",
        "                 hidden_sizes=(32, 32),\n",
        "                 learning_rate=0.001,\n",
        "                 epsilon=0.1,\n",
        "                 replay_size=10000,\n",
        "                 minibatch_size=32,\n",
        "                 target_update=20):\n",
        "\n",
        "        self.state_dims = env.observation_space.shape[0]\n",
        "\n",
        "        # check if the action space has correct type\n",
        "\n",
        "        self.num_actions = env.action_space.n\n",
        "\n",
        "        # create Q-networks for action-value function\n",
        "        self.qnet = QNetwork(self.state_dims, hidden_sizes, self.num_actions, learning_rate)\n",
        "        self.target_qnet = QNetwork(self.state_dims, hidden_sizes, self.num_actions, learning_rate)\n",
        "\n",
        "        # copy weights from Q-network to target Q-network\n",
        "        self.target_qnet.copy_from(self.qnet)\n",
        "\n",
        "        # initialise replay buffer\n",
        "        self.replay_buffer = []\n",
        "        self.replay_size = replay_size\n",
        "        \n",
        "\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.minibatch_size = minibatch_size\n",
        "        self.target_update = target_update\n",
        "        self.target_update_idx = 0\n",
        "\n",
        "    def behaviour(self, state):\n",
        "        # exploratory behaviour policy\n",
        "        if rng.uniform() >= self.epsilon:\n",
        "            # convert state to torch format\n",
        "            if not torch.is_tensor(state):\n",
        "                state = torch.tensor(state, dtype=torch.float)\n",
        "\n",
        "            # exploitation with probability 1-epsilon; break ties randomly\n",
        "            q = self.qnet(state).detach()\n",
        "            j = rng.permutation(self.num_actions)\n",
        "            return j[q[j].argmax().item()]\n",
        "        else:\n",
        "            # exploration with probability epsilon\n",
        "            return self.env.action_space.sample()\n",
        "\n",
        "    def policy(self, state):\n",
        "        # convert state to torch format\n",
        "        if not torch.is_tensor(state):\n",
        "            state = torch.tensor(state, dtype=torch.float)\n",
        "\n",
        "        # greedy policy\n",
        "        q = self.qnet(state).detach()\n",
        "        return q.argmax().item()\n",
        "\n",
        "    def update(self):\n",
        "        # update Q-network if there is enough experience\n",
        "        if len(self.replay_buffer) >= self.minibatch_size:\n",
        "            # select mini-batch of experiences uniformly at random without replacement\n",
        "            batch = rng.choice(len(self.replay_buffer), size=self.minibatch_size, replace=False)\n",
        "\n",
        "            # calculate inputs and targets for the transitions in the mini-batch\n",
        "            inputs = torch.zeros((self.minibatch_size, self.state_dims))\n",
        "            targets = torch.zeros((self.minibatch_size, self.num_actions))\n",
        "\n",
        "            for n, index in enumerate(batch):\n",
        "                state, action, reward, next_state, terminated = self.replay_buffer[index]\n",
        "                # inputs are states\n",
        "                inputs[n, :] = state\n",
        "\n",
        "                # targets are TD targets\n",
        "                targets[n, :] = self.target_qnet(state).detach()\n",
        "\n",
        "                if terminated:\n",
        "                    targets[n, action] = reward\n",
        "                else:\n",
        "                    targets[n, action] = reward + self.gamma*self.target_qnet(next_state).detach().max()\n",
        "\n",
        "            # train Q-network on the mini-batch\n",
        "            self.qnet.update(inputs, targets)\n",
        "\n",
        "        # periodically copy weights from Q-network to target Q-network\n",
        "        self.target_update_idx += 1\n",
        "        if self.target_update_idx % self.target_update == 0:\n",
        "            self.target_qnet.copy_from(self.qnet)\n",
        "\n",
        "    def train(self, max_episodes, stop_criterion, criterion_episodes):\n",
        "        # train the agent for a number of episodes\n",
        "        rewards = []\n",
        "        num_steps = 0\n",
        "        for episode in range(max_episodes):\n",
        "            state, _ = env.reset()\n",
        "            # convert state to torch format\n",
        "            state = torch.tensor(state, dtype=torch.float)\n",
        "            terminated = False\n",
        "            truncated = False\n",
        "            rewards.append(0)\n",
        "            while not (terminated or truncated):\n",
        "                # select action by following behaviour policy\n",
        "                action = self.behaviour(state)\n",
        "\n",
        "                # send the action to the environment\n",
        "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "                # convert next state to torch format and add experience to replay buffer\n",
        "                next_state = torch.tensor(next_state, dtype=torch.float)\n",
        "                \n",
        "                self.replay_buffer.append((state, action, reward, next_state, terminated))\n",
        "                if len(self.replay_buffer)>self.replay_size:\n",
        "                    self.replay_buffer.pop(0)\n",
        "\n",
        "                # update Q-network\n",
        "                self.update()\n",
        "\n",
        "                state = next_state\n",
        "                rewards[-1] += reward\n",
        "                num_steps += 1\n",
        "\n",
        "            print(f'\\rEpisode {episode+1} done: steps = {num_steps}, rewards = {rewards[episode]}     ', end='')\n",
        "\n",
        "            if episode >= criterion_episodes-1 and stop_criterion(rewards[-criterion_episodes:]):\n",
        "                print(f'\\nStopping criterion satisfied after {episode} episodes')\n",
        "                break\n",
        "\n",
        "        # plot rewards received during training\n",
        "        plt.figure(dpi=100)\n",
        "        plt.plot(range(1, len(rewards)+1), rewards, label=f'Rewards')\n",
        "\n",
        "        plt.xlabel('Episodes')\n",
        "        plt.ylabel('Rewards per episode')\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "    def save(self, path):\n",
        "        # save network weights to a file\n",
        "        torch.save(self.qnet.state_dict(), path)\n",
        "\n",
        "    def load(self, path):\n",
        "        # load network weights from a file\n",
        "        self.qnet.load_state_dict(torch.load(path))\n",
        "        self.target_qnet.copy_from(self.qnet)\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.5       , 0.5       , 0.25      , 0.10416667, 0.39933333,\n",
              "       0.5       , 0.        , 0.63753781, 0.61333807, 0.78867355,\n",
              "       0.7692432 , 0.46339552])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env = CarParking()\n",
        "state = env.reset()\n",
        "env.deconstruct_array(state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'CarParking' object has no attribute 'observation_space'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/Users/karmel/Downloads/CAR_SIMULATION_NEW.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION_NEW.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m max_steps \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION_NEW.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m criterion_episodes \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION_NEW.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m agent \u001b[39m=\u001b[39m AgentDQN(env,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION_NEW.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                  gamma\u001b[39m=\u001b[39mgamma,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION_NEW.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m                  hidden_sizes\u001b[39m=\u001b[39mhidden_sizes,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION_NEW.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m                  learning_rate\u001b[39m=\u001b[39mlearning_rate,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION_NEW.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m                  epsilon\u001b[39m=\u001b[39mepsilon,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION_NEW.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m                  replay_size\u001b[39m=\u001b[39mreplay_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION_NEW.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m                  minibatch_size\u001b[39m=\u001b[39mminibatch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION_NEW.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m                  target_update\u001b[39m=\u001b[39mtarget_update)\n",
            "\u001b[1;32m/Users/karmel/Downloads/CAR_SIMULATION_NEW.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION_NEW.ipynb#X13sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, env, gamma,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION_NEW.ipynb#X13sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m              hidden_sizes\u001b[39m=\u001b[39m(\u001b[39m32\u001b[39m, \u001b[39m32\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION_NEW.ipynb#X13sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m              learning_rate\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION_NEW.ipynb#X13sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m              minibatch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION_NEW.ipynb#X13sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m              target_update\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION_NEW.ipynb#X13sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_dims \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION_NEW.ipynb#X13sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     \u001b[39m# check if the action space has correct type\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION_NEW.ipynb#X13sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_actions \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'CarParking' object has no attribute 'observation_space'"
          ]
        }
      ],
      "source": [
        "gamma = 0.99\n",
        "hidden_sizes = (128, 128)\n",
        "learning_rate = 0.001\n",
        "epsilon = 0.2\n",
        "replay_size = 10000\n",
        "minibatch_size = 64\n",
        "target_update = 20\n",
        "max_episodes = 100\n",
        "max_steps = 1000\n",
        "criterion_episodes = 5\n",
        "\n",
        "agent = AgentDQN(env,\n",
        "                 gamma=gamma,\n",
        "                 hidden_sizes=hidden_sizes,\n",
        "                 learning_rate=learning_rate,\n",
        "                 epsilon=epsilon,\n",
        "                 replay_size=replay_size,\n",
        "                 minibatch_size=minibatch_size,\n",
        "                 target_update=target_update)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgGt9nozNxit",
        "outputId": "81b6da20-932b-4446-de79-616fdfe3ebf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -10000: avg score: -10000.0\n",
            "latest score: -822.0701996816686: avg score: -9791.410686356401\n",
            "latest score: -10000: avg score: -9796.04600443737\n",
            "latest score: -10000: avg score: -9800.479786949601\n",
            "latest score: -10000: avg score: -9804.724897865568\n",
            "latest score: -10000: avg score: -9808.793129160034\n",
            "latest score: -10000: avg score: -9812.695310197585\n",
            "latest score: -10000: avg score: -9816.441403993633\n",
            "latest score: -10000: avg score: -9820.04059215062\n",
            "latest score: -10000: avg score: -9823.501349993878\n",
            "latest score: -10000: avg score: -9826.831513201541\n",
            "latest score: -10000: avg score: -9830.038337031143\n",
            "latest score: -10000: avg score: -9833.12854908512\n",
            "latest score: -10000: avg score: -9836.108396422887\n",
            "latest score: -10000: avg score: -9838.983687713713\n",
            "latest score: -10000: avg score: -9841.759831028994\n",
            "latest score: -10000: avg score: -9844.441867791214\n",
            "latest score: -10000: avg score: -9847.034503328028\n",
            "latest score: -10000: avg score: -9849.542134421012\n",
            "latest score: -10000: avg score: -9851.968874188415\n",
            "latest score: -10000: avg score: -9854.318574598123\n",
            "latest score: -10000: avg score: -9856.594846870026\n",
            "latest score: -10000: avg score: -9858.801079995103\n",
            "latest score: -10000: avg score: -9860.940457570934\n",
            "latest score: -10000: avg score: -9863.015973129577\n",
            "latest score: -10000: avg score: -9865.030444112967\n",
            "latest score: -10000: avg score: -9866.986524633068\n",
            "latest score: -10000: avg score: -9868.88671713831\n",
            "latest score: -10000: avg score: -9870.733383094108\n",
            "latest score: -10000: avg score: -9872.528752773356\n",
            "latest score: -10000: avg score: -9874.274934242214\n",
            "latest score: -10000: avg score: -9875.97392161732\n",
            "latest score: -10000: avg score: -9877.627602662422\n",
            "latest score: -10000: avg score: -9879.237765785285\n",
            "latest score: -10000: avg score: -9880.806106489372\n",
            "latest score: -10000: avg score: -9882.334233329253\n",
            "latest score: -10000: avg score: -9883.823673413692\n",
            "latest score: -10000: avg score: -9885.27587749602\n",
            "latest score: -10000: avg score: -9886.692224687427\n",
            "latest score: -10000: avg score: -9888.074026825387\n",
            "latest score: -10000: avg score: -9889.422532526285\n",
            "latest score: -10000: avg score: -9890.738930948592\n",
            "latest score: -10000: avg score: -9892.024355290372\n",
            "latest score: -10000: avg score: -9893.27988604281\n",
            "latest score: -10000: avg score: -9894.50655401933\n",
            "latest score: -10000: avg score: -9895.7053431782\n",
            "latest score: -10000: avg score: -9896.87719325485\n",
            "latest score: -10000: avg score: -9898.023002218686\n",
            "latest score: 77.60482635326828: avg score: -9788.400718388224\n",
            "latest score: -10000: avg score: -9790.700710579657\n",
            "latest score: -10000: avg score: -9792.951240573424\n",
            "latest score: -10000: avg score: -9795.153886950302\n",
            "latest score: -10000: avg score: -9797.31016182451\n",
            "latest score: -10000: avg score: -9799.421514305504\n",
            "latest score: -10000: avg score: -9801.489333745654\n",
            "latest score: -10000: avg score: -9803.514952789066\n",
            "latest score: -10000: avg score: -9805.49965023564\n",
            "latest score: -10000: avg score: -9807.444653733284\n",
            "latest score: -10000: avg score: -9809.351142310183\n",
            "latest score: -10000: avg score: -9811.220248758122\n",
            "latest score: -10000: avg score: -9813.053061876975\n",
            "latest score: -10000: avg score: -9814.850628589696\n",
            "latest score: -10000: avg score: -9816.613955936462\n",
            "latest score: -10000: avg score: -9818.344012955928\n",
            "latest score: -10000: avg score: -9820.041732461013\n",
            "latest score: -10000: avg score: -9821.708012716004\n",
            "latest score: -10000: avg score: -9823.34371902136\n",
            "latest score: -10000: avg score: -9824.949685212077\n",
            "latest score: -10000: avg score: -9826.526715075031\n",
            "latest score: -10000: avg score: -9828.075583690432\n",
            "latest score: -10000: avg score: -9829.59703870202\n",
            "latest score: -10000: avg score: -9831.091801520424\n",
            "latest score: -10000: avg score: -9832.560568463725\n",
            "latest score: -10000: avg score: -9834.004011839039\n",
            "latest score: -10000: avg score: -9835.422780968618\n",
            "latest score: -10000: avg score: -9836.8175031638\n",
            "latest score: -10000: avg score: -9838.18878464982\n",
            "latest score: -10000: avg score: -9839.537211444404\n",
            "latest score: -10000: avg score: -9840.863350192796\n",
            "latest score: -10000: avg score: -9842.167748961709\n",
            "latest score: -10000: avg score: -9843.45093799454\n",
            "latest score: -10000: avg score: -9844.713430430067\n",
            "latest score: -10000: avg score: -9845.955722986628\n",
            "latest score: -10000: avg score: -9847.178296613718\n",
            "latest score: -10000: avg score: -9848.381617112822\n",
            "latest score: -10000: avg score: -9849.566135729128\n",
            "latest score: -10000: avg score: -9850.732289715725\n",
            "latest score: -10000: avg score: -9851.880502871758\n",
            "latest score: -10000: avg score: -9853.011186055943\n",
            "latest score: -10000: avg score: -9854.12473767673\n",
            "latest score: -10000: avg score: -9855.221544160364\n",
            "latest score: -10000: avg score: -9856.301980397973\n",
            "latest score: -10000: avg score: -9857.366410172803\n",
            "latest score: -10000: avg score: -9858.41518656859\n",
            "latest score: -10000: avg score: -9859.44865236006\n",
            "latest score: -10000: avg score: -9860.467140386438\n",
            "latest score: -10000: avg score: -9861.470973908838\n",
            "latest score: -10000: avg score: -9862.460466952345\n",
            "latest score: -10000: avg score: -9863.435924633535\n",
            "latest score: -10000: avg score: -9864.397643474143\n",
            "latest score: -10000: avg score: -9865.345911701597\n",
            "latest score: -10000: avg score: -9866.281009537002\n",
            "latest score: -10000: avg score: -9867.20320947123\n",
            "latest score: -10000: avg score: -9868.112776529646\n",
            "latest score: -10000: avg score: -9869.009968526043\n",
            "latest score: -10000: avg score: -9869.895036306272\n",
            "latest score: -10000: avg score: -9870.76822398207\n",
            "latest score: -10000: avg score: -9871.629769155523\n",
            "latest score: -10000: avg score: -9872.479903134625\n",
            "latest score: -10000: avg score: -9873.318851140319\n",
            "latest score: -654.4678724567223: avg score: -9813.064923175065\n",
            "latest score: -10000: avg score: -9814.278787310293\n",
            "latest score: -10000: avg score: -9815.476988682485\n",
            "latest score: -10000: avg score: -9816.659828498623\n",
            "latest score: -10000: avg score: -9817.827600291625\n",
            "latest score: -10000: avg score: -9818.980590163197\n",
            "latest score: -10000: avg score: -9820.119077017516\n",
            "latest score: -10000: avg score: -9821.243332786156\n",
            "latest score: -10000: avg score: -9822.353622644629\n",
            "latest score: -10000: avg score: -9823.450205220895\n",
            "latest score: -10000: avg score: -9824.533332796227\n",
            "latest score: -10000: avg score: -9825.60325149869\n",
            "latest score: -10000: avg score: -9826.660201489607\n",
            "latest score: -10000: avg score: -9827.704417143284\n",
            "latest score: -10000: avg score: -9828.73612722027\n",
            "latest score: -10000: avg score: -9829.755555034435\n",
            "latest score: -10000: avg score: -9830.762918614113\n",
            "latest score: -10000: avg score: -9831.758430857559\n",
            "latest score: -10000: avg score: -9832.742299682954\n",
            "latest score: -10000: avg score: -9833.714728173169\n",
            "latest score: -10000: avg score: -9834.67591471552\n",
            "latest score: -10000: avg score: -9835.626053136695\n",
            "latest score: -10000: avg score: -9836.565332833057\n",
            "latest score: -10000: avg score: -9837.493938896507\n",
            "latest score: -10000: avg score: -9838.412052236074\n",
            "latest score: -10000: avg score: -9839.319849695421\n",
            "latest score: -10000: avg score: -9840.217504166398\n",
            "latest score: -10000: avg score: -9841.105184698807\n",
            "latest score: -10000: avg score: -9841.983056606547\n",
            "latest score: -10000: avg score: -9842.851281570247\n",
            "latest score: -10000: avg score: -9843.71001773653\n",
            "latest score: 104.27991504457442: avg score: -9789.644855058372\n",
            "latest score: -10000: avg score: -9790.781909895895\n",
            "latest score: -10000: avg score: -9791.906738337315\n",
            "latest score: -10000: avg score: -9793.019536528023\n",
            "latest score: -10000: avg score: -9794.12049644011\n",
            "latest score: -10000: avg score: -9795.209805982755\n",
            "latest score: -10000: avg score: -9796.287649109161\n",
            "latest score: -10000: avg score: -9797.354205920108\n",
            "latest score: -10000: avg score: -9798.409652764274\n",
            "latest score: -10000: avg score: -9799.454162335443\n",
            "latest score: -10000: avg score: -9800.487903766703\n",
            "latest score: -10000: avg score: -9801.511042721746\n",
            "latest score: -10000: avg score: -9802.52374148337\n",
            "latest score: -10000: avg score: -9803.526159039291\n",
            "latest score: -10000: avg score: -9804.518451165357\n",
            "latest score: -10000: avg score: -9805.500770506234\n",
            "latest score: -10000: avg score: -9806.473266653702\n",
            "latest score: -10000: avg score: -9807.43608622259\n",
            "latest score: -10000: avg score: -9808.389372924457\n",
            "latest score: -10000: avg score: -9809.333267639116\n",
            "latest score: -10000: avg score: -9810.267908484022\n",
            "latest score: -10000: avg score: -9811.193430881662\n",
            "latest score: -10000: avg score: -9812.109967624954\n",
            "latest score: -10000: avg score: -9813.017648940775\n",
            "latest score: -10000: avg score: -9813.916602551637\n",
            "latest score: -10000: avg score: -9814.8069537356\n",
            "latest score: -10000: avg score: -9815.688825384479\n",
            "latest score: -10000: avg score: -9816.562338060381\n",
            "latest score: -10000: avg score: -9817.427610050663\n",
            "latest score: -10000: avg score: -9818.284757421317\n",
            "latest score: -10000: avg score: -9819.13389406888\n",
            "latest score: -10000: avg score: -9819.975131770887\n",
            "latest score: -10000: avg score: -9820.80858023491\n",
            "latest score: -10000: avg score: -9821.63434714627\n",
            "latest score: -10000: avg score: -9822.452538214406\n",
            "latest score: -10000: avg score: -9823.263257217994\n",
            "latest score: -10000: avg score: -9824.06660604882\n",
            "latest score: -10000: avg score: -9824.862684754482\n",
            "latest score: -10000: avg score: -9825.651591579912\n",
            "latest score: -10000: avg score: -9826.433423007806\n",
            "latest score: -10000: avg score: -9827.208273797949\n",
            "latest score: -10000: avg score: -9827.976237025514\n",
            "latest score: -10000: avg score: -9828.737404118321\n",
            "latest score: -10000: avg score: -9829.49186489313\n",
            "latest score: -10000: avg score: -9830.239707590967\n",
            "latest score: -10000: avg score: -9830.98101891153\n",
            "latest score: -10000: avg score: -9831.715884046698\n",
            "latest score: -10000: avg score: -9832.444386713163\n",
            "latest score: -10000: avg score: -9833.166609184227\n",
            "latest score: -10000: avg score: -9833.882632320774\n",
            "latest score: -10000: avg score: -9834.592535601456\n",
            "latest score: -10000: avg score: -9835.296397152088\n",
            "latest score: -10000: avg score: -9835.994293774324\n",
            "latest score: 97.81926601257838: avg score: -9794.079468627542\n",
            "latest score: -10000: avg score: -9794.944680944234\n",
            "latest score: -10000: avg score: -9795.802652990493\n",
            "latest score: -10000: avg score: -9796.653475269699\n",
            "latest score: -10000: avg score: -9797.497236783101\n",
            "latest score: -10000: avg score: -9798.334025060858\n",
            "latest score: -10000: avg score: -9799.163926192296\n",
            "latest score: -10000: avg score: -9799.987024855442\n",
            "latest score: -10000: avg score: -9800.803404345826\n",
            "latest score: -10000: avg score: -9801.613146604584\n",
            "latest score: -10000: avg score: -9802.41633224586\n",
            "latest score: -10000: avg score: -9803.21304058358\n",
            "latest score: -10000: avg score: -9804.003349657542\n",
            "latest score: -10000: avg score: -9804.78733625891\n",
            "latest score: -10000: avg score: -9805.56507595509\n",
            "latest score: -10000: avg score: -9806.336643113998\n",
            "latest score: -10000: avg score: -9807.102110927777\n",
            "latest score: -10000: avg score: -9807.861551435935\n",
            "latest score: -10000: avg score: -9808.615035547951\n",
            "latest score: -10000: avg score: -9809.362633065342\n",
            "latest score: -10000: avg score: -9810.10441270322\n",
            "latest score: -10000: avg score: -9810.840442111348\n",
            "latest score: -10000: avg score: -9811.570787894701\n",
            "latest score: -10000: avg score: -9812.295515633568\n",
            "latest score: -10000: avg score: -9813.014689903172\n",
            "latest score: -10000: avg score: -9813.728374292854\n",
            "latest score: -10000: avg score: -9814.43663142482\n",
            "latest score: -10000: avg score: -9815.139522972453\n",
            "latest score: -10000: avg score: -9815.837109678218\n",
            "latest score: -10000: avg score: -9816.529451371156\n",
            "latest score: -10000: avg score: -9817.216606983999\n",
            "latest score: -10000: avg score: -9817.89863456988\n",
            "latest score: -527.7642545181416: avg score: -9783.362819030654\n",
            "latest score: -10000: avg score: -9784.165178960171\n",
            "latest score: -10000: avg score: -9784.961617414192\n",
            "latest score: -10000: avg score: -9785.75219970311\n",
            "latest score: -10000: avg score: -9786.536990180388\n",
            "latest score: -10000: avg score: -9787.316052260021\n",
            "latest score: -10000: avg score: -9788.089448433622\n",
            "latest score: -10000: avg score: -9788.857240287123\n",
            "latest score: -10000: avg score: -9789.619488517134\n",
            "latest score: -714.6952325743067: avg score: -9756.975876085684\n",
            "latest score: -10000: avg score: -9757.846930293263\n",
            "latest score: -913.8641058163828: avg score: -9726.261277348704\n",
            "latest score: -10000: avg score: -9727.235436504045\n",
            "latest score: -10000: avg score: -9728.202686729208\n",
            "latest score: -10000: avg score: -9729.163101263734\n",
            "latest score: -10000: avg score: -9730.116752315622\n",
            "latest score: -10000: avg score: -9731.063711079427\n",
            "latest score: -10000: avg score: -9732.004047753975\n",
            "latest score: -10000: avg score: -9732.93783155971\n",
            "latest score: -10000: avg score: -9733.865130755683\n",
            "latest score: -10000: avg score: -9734.786012656183\n",
            "latest score: -10000: avg score: -9735.700543647023\n",
            "latest score: -10000: avg score: -9736.6087892015\n",
            "latest score: -10000: avg score: -9737.510813896017\n",
            "latest score: -10000: avg score: -9738.406681425382\n",
            "latest score: -10000: avg score: -9739.296454617812\n",
            "latest score: -10000: avg score: -9740.180195449617\n",
            "latest score: -10000: avg score: -9741.057965059585\n",
            "latest score: -10000: avg score: -9741.929823763086\n",
            "latest score: -10000: avg score: -9742.795831065896\n",
            "latest score: -10000: avg score: -9743.656045677715\n",
            "latest score: -10000: avg score: -9744.510525525457\n",
            "latest score: -10000: avg score: -9745.359327766235\n",
            "latest score: -10000: avg score: -9746.202508800121\n",
            "latest score: -10000: avg score: -9747.04012428263\n",
            "latest score: -10000: avg score: -9747.872229136963\n",
            "latest score: -10000: avg score: -9748.698877566023\n",
            "latest score: 133.7620719101346: avg score: -9716.403253548127\n",
            "latest score: -10000: avg score: -9717.32702145188\n",
            "latest score: -10000: avg score: -9718.24479086275\n",
            "latest score: -722.1239896227939: avg score: -9689.131131311811\n",
            "latest score: -10000: avg score: -9690.13393411403\n",
            "latest score: -10000: avg score: -9691.130288023633\n",
            "latest score: -10000: avg score: -9692.120255049198\n",
            "latest score: -253.74283519542607: avg score: -9661.965694602379\n",
            "latest score: -10000: avg score: -9663.042236976256\n",
            "latest score: -10000: avg score: -9664.11194416046\n",
            "latest score: -10000: avg score: -9665.174881046029\n",
            "latest score: -10000: avg score: -9666.231111705189\n",
            "latest score: -10000: avg score: -9667.28069940423\n",
            "latest score: -10000: avg score: -9668.323706616127\n",
            "latest score: -865.2858317254456: avg score: -9640.814213257094\n",
            "latest score: -10000: avg score: -9641.9331720943\n",
            "latest score: -10000: avg score: -9643.045180876616\n",
            "latest score: -818.2693233032848: avg score: -9615.723893391869\n",
            "latest score: -10000: avg score: -9616.909930757942\n",
            "latest score: -10000: avg score: -9618.088669432533\n",
            "latest score: -10000: avg score: -9619.260176581514\n",
            "latest score: -10000: avg score: -9620.424518549155\n",
            "latest score: -10000: avg score: -9621.581760870651\n",
            "latest score: -10000: avg score: -9622.731968284417\n",
            "latest score: -10000: avg score: -9623.875204744161\n",
            "latest score: -10000: avg score: -9625.011533430736\n",
            "latest score: -10000: avg score: -9626.141016763775\n",
            "latest score: -10000: avg score: -9627.263716413134\n",
            "latest score: -10000: avg score: -9628.3796933101\n",
            "latest score: -10000: avg score: -9629.489007658429\n",
            "latest score: -10000: avg score: -9630.59171894516\n",
            "latest score: -10000: avg score: -9631.687885951256\n",
            "latest score: -10000: avg score: -9632.777566762052\n",
            "latest score: -10000: avg score: -9633.860818777503\n",
            "latest score: -10000: avg score: -9634.937698722275\n",
            "latest score: -10000: avg score: -9636.00826265564\n",
            "latest score: -10000: avg score: -9637.072565981209\n",
            "latest score: -10000: avg score: -9638.130663456483\n",
            "latest score: -10000: avg score: -9639.182609202247\n",
            "latest score: -10000: avg score: -9640.228456711808\n",
            "latest score: -10000: avg score: -9641.268258860038\n",
            "latest score: -10000: avg score: -9642.302067912315\n",
            "latest score: -10000: avg score: -9643.329935533257\n",
            "latest score: -10000: avg score: -9644.351912795339\n",
            "latest score: -10000: avg score: -9645.368050187353\n",
            "latest score: -10000: avg score: -9646.378397622717\n",
            "latest score: -10000: avg score: -9647.38300444765\n",
            "latest score: -10000: avg score: -9648.381919449217\n",
            "latest score: -10000: avg score: -9649.375190863202\n",
            "latest score: -10000: avg score: -9650.362866381896\n",
            "latest score: -10000: avg score: -9651.344993161723\n",
            "latest score: -10000: avg score: -9652.321617830738\n",
            "latest score: -10000: avg score: -9653.292786496015\n",
            "latest score: -10000: avg score: -9654.258544750901\n",
            "latest score: -10000: avg score: -9655.218937682148\n",
            "latest score: -10000: avg score: -9656.174009876935\n",
            "latest score: -10000: avg score: -9657.123805429761\n",
            "latest score: -10000: avg score: -9658.068367949238\n",
            "latest score: -10000: avg score: -9659.007740564763\n",
            "latest score: -10000: avg score: -9659.941965933078\n",
            "latest score: -10000: avg score: -9660.871086244735\n",
            "latest score: -10000: avg score: -9661.795143230445\n",
            "latest score: -10000: avg score: -9662.714178167318\n",
            "latest score: -10000: avg score: -9663.628231885023\n",
            "latest score: -10000: avg score: -9664.53734477182\n",
            "latest score: -10000: avg score: -9665.44155678052\n",
            "latest score: -10000: avg score: -9666.340907434336\n",
            "latest score: -10000: avg score: -9667.235435832636\n",
            "latest score: -10000: avg score: -9668.125180656612\n",
            "latest score: -10000: avg score: -9669.010180174862\n",
            "latest score: -10000: avg score: -9669.890472248866\n",
            "latest score: -10000: avg score: -9670.76609433839\n",
            "latest score: -10000: avg score: -9671.637083506808\n",
            "latest score: -10000: avg score: -9672.503476426315\n",
            "latest score: -10000: avg score: -9673.365309383087\n",
            "latest score: -10000: avg score: -9674.222618282345\n",
            "latest score: -10000: avg score: -9675.075438653334\n",
            "latest score: -10000: avg score: -9675.923805654238\n",
            "latest score: -10000: avg score: -9676.767754077015\n",
            "latest score: -10000: avg score: -9677.607318352138\n",
            "latest score: -10000: avg score: -9678.4425325533\n",
            "latest score: -10000: avg score: -9679.273430401998\n",
            "latest score: -10000: avg score: -9680.100045272096\n",
            "latest score: -10000: avg score: -9680.922410194276\n",
            "latest score: -10000: avg score: -9681.740557860445\n",
            "latest score: -10000: avg score: -9682.554520628066\n",
            "latest score: -10000: avg score: -9683.364330524422\n",
            "latest score: -10000: avg score: -9684.170019250823\n",
            "latest score: -10000: avg score: -9684.971618186735\n",
            "latest score: -10000: avg score: -9685.769158393856\n",
            "latest score: -10000: avg score: -9686.562670620135\n",
            "latest score: -10000: avg score: -9687.352185303711\n",
            "latest score: -10000: avg score: -9688.137732576817\n",
            "latest score: -10000: avg score: -9688.919342269608\n",
            "latest score: -10000: avg score: -9689.697043913933\n",
            "latest score: -10000: avg score: -9690.470866747066\n",
            "latest score: -10000: avg score: -9691.240839715356\n",
            "latest score: -10000: avg score: -9692.006991477849\n",
            "latest score: -10000: avg score: -9692.769350409835\n",
            "latest score: -10000: avg score: -9693.527944606354\n",
            "latest score: -10000: avg score: -9694.282801885649\n",
            "latest score: -10000: avg score: -9695.033949792563\n",
            "latest score: -10000: avg score: -9695.781415601896\n",
            "latest score: -10000: avg score: -9696.525226321695\n",
            "latest score: -10000: avg score: -9697.265408696521\n",
            "latest score: -10000: avg score: -9698.001989210641\n",
            "latest score: -10000: avg score: -9698.734994091197\n",
            "latest score: -10000: avg score: -9699.464449311316\n",
            "latest score: -10000: avg score: -9700.190380593172\n",
            "latest score: -10000: avg score: -9700.91281341102\n",
            "latest score: -10000: avg score: -9701.631772994167\n",
            "latest score: -10000: avg score: -9702.347284329913\n",
            "latest score: -10000: avg score: -9703.059372166445\n",
            "latest score: -10000: avg score: -9703.76806101569\n",
            "latest score: -10000: avg score: -9704.473375156129\n",
            "latest score: -10000: avg score: -9705.175338635567\n",
            "latest score: -10000: avg score: -9705.873975273871\n",
            "latest score: -10000: avg score: -9706.56930866566\n",
            "latest score: -10000: avg score: -9707.261362182957\n",
            "latest score: -10000: avg score: -9707.95015897782\n",
            "latest score: -10000: avg score: -9708.635721984914\n",
            "latest score: -10000: avg score: -9709.31807392406\n",
            "latest score: -10000: avg score: -9709.997237302741\n",
            "latest score: -10000: avg score: -9710.673234418586\n",
            "latest score: -10000: avg score: -9711.346087361799\n",
            "latest score: -10000: avg score: -9712.015818017571\n",
            "latest score: -10000: avg score: -9712.682448068457\n",
            "latest score: -431.7310670156548: avg score: -9691.248380213832\n",
            "latest score: -10000: avg score: -9691.959789476012\n",
            "latest score: -10000: avg score: -9692.667927891009\n",
            "latest score: -10000: avg score: -9693.372817964653\n",
            "latest score: -10000: avg score: -9694.074481996771\n",
            "latest score: -10000: avg score: -9694.772942083537\n",
            "latest score: -10000: avg score: -9695.468220119794\n",
            "latest score: -10000: avg score: -9696.160337801339\n",
            "latest score: -10000: avg score: -9696.849316627186\n",
            "latest score: -10000: avg score: -9697.535177901786\n",
            "latest score: -10000: avg score: -9698.217942737221\n",
            "latest score: -10000: avg score: -9698.897632055381\n",
            "latest score: -10000: avg score: -9699.574266590089\n",
            "latest score: -10000: avg score: -9700.247866889213\n",
            "latest score: -10000: avg score: -9700.918453316754\n",
            "latest score: -10000: avg score: -9701.586046054887\n",
            "latest score: -10000: avg score: -9702.250665105988\n",
            "latest score: -10000: avg score: -9702.912330294643\n",
            "latest score: -10000: avg score: -9703.571061269598\n",
            "latest score: -10000: avg score: -9704.22687750573\n",
            "latest score: -10000: avg score: -9704.879798305936\n",
            "latest score: -10000: avg score: -9705.52984280306\n",
            "latest score: -10000: avg score: -9706.177029961735\n",
            "latest score: -10000: avg score: -9706.82137858024\n",
            "latest score: -10000: avg score: -9707.462907292318\n",
            "latest score: -10000: avg score: -9708.101634568971\n",
            "latest score: -10000: avg score: -9708.737578720238\n",
            "latest score: -10000: avg score: -9709.370757896933\n",
            "latest score: -10000: avg score: -9710.001190092384\n",
            "latest score: -10000: avg score: -9710.628893144132\n",
            "latest score: -10000: avg score: -9711.253884735614\n",
            "latest score: -10000: avg score: -9711.876182397822\n",
            "latest score: -10000: avg score: -9712.495803510945\n",
            "latest score: -10000: avg score: -9713.112765305985\n",
            "latest score: -10000: avg score: -9713.727084866357\n",
            "latest score: -10000: avg score: -9714.338779129464\n",
            "latest score: -10000: avg score: -9714.94786488825\n",
            "latest score: -10000: avg score: -9715.554358792742\n",
            "latest score: -10000: avg score: -9716.15827735157\n",
            "latest score: -10000: avg score: -9716.759636933451\n",
            "latest score: -10000: avg score: -9717.358453768687\n",
            "latest score: -10000: avg score: -9717.95474395061\n",
            "latest score: -10000: avg score: -9718.54852343703\n",
            "latest score: -10000: avg score: -9719.139808051657\n",
            "latest score: -10000: avg score: -9719.728613485511\n",
            "latest score: -10000: avg score: -9720.314955298303\n",
            "latest score: -10000: avg score: -9720.898848919811\n",
            "latest score: -10000: avg score: -9721.480309651228\n",
            "latest score: -10000: avg score: -9722.059352666505\n",
            "latest score: -10000: avg score: -9722.63599301367\n",
            "latest score: -10000: avg score: -9723.210245616126\n",
            "latest score: -10000: avg score: -9723.782125273945\n",
            "latest score: -10000: avg score: -9724.351646665133\n",
            "latest score: -869.1088060977105: avg score: -9706.13098238413\n",
            "latest score: -10000: avg score: -9706.734409525025\n",
            "latest score: -10000: avg score: -9707.335363603865\n",
            "latest score: -10000: avg score: -9707.933859792816\n",
            "latest score: -10000: avg score: -9708.529913140177\n",
            "latest score: -10000: avg score: -9709.123538571665\n",
            "latest score: -513.7303823545323: avg score: -9690.433715083416\n",
            "latest score: -10000: avg score: -9691.061638582234\n",
            "latest score: -10000: avg score: -9691.68701988065\n",
            "latest score: -10000: avg score: -9692.309874385941\n",
            "latest score: -10000: avg score: -9692.930217381132\n",
            "latest score: -10000: avg score: -9693.54806402624\n",
            "latest score: -10000: avg score: -9694.16342935952\n",
            "latest score: -10000: avg score: -9694.77632829868\n",
            "latest score: -10000: avg score: -9695.386775642082\n",
            "latest score: -10000: avg score: -9695.994786069943\n",
            "latest score: -10000: avg score: -9696.6003741455\n",
            "latest score: -10000: avg score: -9697.203554316186\n",
            "latest score: -10000: avg score: -9697.804340914763\n",
            "latest score: -10000: avg score: -9698.402748160477\n",
            "latest score: -10000: avg score: -9698.99879016016\n",
            "latest score: -10000: avg score: -9699.59248090935\n",
            "latest score: -10000: avg score: -9700.183834293388\n",
            "latest score: -10000: avg score: -9700.772864088489\n",
            "latest score: -10000: avg score: -9701.359583962825\n",
            "latest score: -10000: avg score: -9701.944007477576\n",
            "latest score: -10000: avg score: -9702.52614808797\n",
            "latest score: -10000: avg score: -9703.10601914433\n",
            "latest score: -10000: avg score: -9703.683633893075\n",
            "latest score: -10000: avg score: -9704.25900547775\n",
            "latest score: -10000: avg score: -9704.832146940002\n",
            "latest score: -10000: avg score: -9705.403071220582\n",
            "latest score: -10000: avg score: -9705.971791160311\n",
            "latest score: -10000: avg score: -9706.538319501042\n",
            "latest score: -10000: avg score: -9707.102668886617\n",
            "latest score: -10000: avg score: -9707.664851863803\n",
            "latest score: -10000: avg score: -9708.224880883221\n",
            "latest score: -10000: avg score: -9708.782768300269\n",
            "latest score: -10000: avg score: -9709.338526376032\n",
            "latest score: -10000: avg score: -9709.892167278173\n",
            "latest score: -10000: avg score: -9710.443703081826\n",
            "latest score: -10000: avg score: -9710.993145770477\n",
            "latest score: -10000: avg score: -9711.54050723682\n",
            "latest score: -10000: avg score: -9712.085799283632\n",
            "latest score: -10000: avg score: -9712.629033624606\n",
            "latest score: -10000: avg score: -9713.1702218852\n",
            "latest score: -10000: avg score: -9713.709375603461\n",
            "latest score: -10000: avg score: -9714.246506230846\n",
            "latest score: -10000: avg score: -9714.781625133035\n",
            "latest score: -10000: avg score: -9715.31474359073\n",
            "latest score: -10000: avg score: -9715.84587280045\n",
            "latest score: -10000: avg score: -9716.37502387531\n",
            "latest score: -10000: avg score: -9716.902207845802\n",
            "latest score: -10000: avg score: -9717.427435660558\n",
            "latest score: -10000: avg score: -9717.950718187112\n",
            "latest score: -10000: avg score: -9718.472066212646\n",
            "latest score: -10000: avg score: -9718.991490444725\n",
            "latest score: -10000: avg score: -9719.509001512046\n",
            "latest score: -10000: avg score: -9720.024609965149\n",
            "latest score: -10000: avg score: -9720.53832627714\n",
            "latest score: -10000: avg score: -9721.050160844397\n",
            "latest score: -10000: avg score: -9721.560123987278\n",
            "latest score: -10000: avg score: -9722.068225950805\n",
            "latest score: -10000: avg score: -9722.574476905356\n",
            "latest score: -10000: avg score: -9723.078886947347\n",
            "latest score: -10000: avg score: -9723.581466099893\n",
            "latest score: -10000: avg score: -9724.08222431348\n",
            "latest score: -10000: avg score: -9724.58117146662\n",
            "latest score: -10000: avg score: -9725.0783173665\n",
            "latest score: -10000: avg score: -9725.573671749624\n",
            "latest score: -10000: avg score: -9726.067244282447\n",
            "latest score: -10000: avg score: -9726.559044562013\n",
            "latest score: -10000: avg score: -9727.04908211656\n",
            "latest score: -10000: avg score: -9727.537366406155\n",
            "latest score: -10000: avg score: -9728.023906823288\n",
            "latest score: -10000: avg score: -9728.508712693478\n",
            "latest score: -10000: avg score: -9728.991793275874\n",
            "latest score: -10000: avg score: -9729.473157763838\n",
            "latest score: -10000: avg score: -9729.952815285535\n",
            "latest score: -10000: avg score: -9730.430774904498\n",
            "latest score: -10000: avg score: -9730.907045620213\n",
            "latest score: -10000: avg score: -9731.381636368678\n",
            "latest score: -10000: avg score: -9731.85455602296\n",
            "latest score: -10000: avg score: -9732.325813393745\n",
            "latest score: -10000: avg score: -9732.795417229896\n",
            "latest score: -10000: avg score: -9733.263376218985\n",
            "latest score: -10000: avg score: -9733.729698987834\n",
            "latest score: -10000: avg score: -9734.194394103039\n",
            "latest score: -10000: avg score: -9734.657470071501\n",
            "latest score: -10000: avg score: -9735.11893534094\n",
            "latest score: -10000: avg score: -9735.57879830042\n",
            "latest score: -10000: avg score: -9736.037067280833\n",
            "latest score: -10000: avg score: -9736.493750555434\n",
            "latest score: -10000: avg score: -9736.948856340312\n",
            "latest score: -10000: avg score: -9737.402392794898\n",
            "latest score: -10000: avg score: -9737.854368022447\n",
            "latest score: -10000: avg score: -9738.304790070517\n",
            "latest score: -10000: avg score: -9738.75366693146\n",
            "latest score: -10000: avg score: -9739.201006542879\n",
            "latest score: -10000: avg score: -9739.646816788105\n",
            "latest score: -10000: avg score: -9740.091105496656\n",
            "latest score: -10000: avg score: -9740.533880444704\n",
            "latest score: -10000: avg score: -9740.975149355512\n",
            "latest score: -10000: avg score: -9741.4149198999\n",
            "latest score: -10000: avg score: -9741.85319969668\n",
            "latest score: -10000: avg score: -9742.289996313099\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/karmel/Downloads/CAR_SIMULATION.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION.ipynb#W4sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION.ipynb#W4sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     agent\u001b[39m.\u001b[39mstore_transition(observation_deconstructed, action, reward, env\u001b[39m.\u001b[39mdeconstruct_array(observation_), done)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION.ipynb#W4sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     agent\u001b[39m.\u001b[39mlearn()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     observation \u001b[39m=\u001b[39m observation_\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION.ipynb#W4sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m scores\u001b[39m.\u001b[39mappend(score)\n",
            "\u001b[1;32m/Users/karmel/Downloads/CAR_SIMULATION.ipynb Cell 6\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION.ipynb#W4sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m q_next \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mQ_eval\u001b[39m.\u001b[39mforward(new_state_batch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION.ipynb#W4sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m q_next[terminal_batch] \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION.ipynb#W4sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m q_target \u001b[39m=\u001b[39m reward_batch \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mmax(q_next, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION.ipynb#W4sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mQ_eval\u001b[39m.\u001b[39mloss(q_target, q_eval)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karmel/Downloads/CAR_SIMULATION.ipynb#W4sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "env = CarParking()\n",
        "state = env.reset()\n",
        "\n",
        "n_observations = len(env.deconstruct_array(state))\n",
        "agent = Agent(gamma=0.99, epsilon=1.0, batch_size=64, n_actions=9, eps_end = 0.01, input_dims=[n_observations], lr=0.03)\n",
        "scores, eps_history = [], []\n",
        "n_games=10000\n",
        "\n",
        "for i in range(n_games):\n",
        "    score = 0\n",
        "    turn = 0\n",
        "    observation = env.reset()\n",
        "    observation_deconstructed = env.deconstruct_array(observation)\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.choose_action(observation_deconstructed)\n",
        "        observation_,reward, done, info, coords = env.step(action)\n",
        "        turn += 1\n",
        "        if turn == max_loop:\n",
        "            done = True\n",
        "        if reward <= -1000:\n",
        "\n",
        "            score = reward\n",
        "        else:\n",
        "            score += reward\n",
        "        agent.store_transition(observation_deconstructed, action, reward, env.deconstruct_array(observation_), done)\n",
        "        agent.learn()\n",
        "        observation = observation_\n",
        "    scores.append(score)\n",
        "    eps_history.append(agent.epsilon)\n",
        "    avg_score = np.mean(scores)\n",
        "    print(f'latest score: {score}: avg score: {avg_score}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "Qs7ZzrCaevyE",
        "outputId": "14444fc3-dcf4-422a-fa7f-c38cb9e63274"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CAR CRASH\n",
            "2.3230268955230713\n"
          ]
        }
      ],
      "source": [
        "env = CarParking()\n",
        "state = env.reset()\n",
        "score = 0\n",
        "start = time.time()\n",
        "for i in range(10000):\n",
        "    action = env.action_space.sample()\n",
        "    action = np.array([3,1])\n",
        "    if i <1000:\n",
        "        action = np.array([3,1])\n",
        "    # if i > 20 and i < 60:\n",
        "    #     action = np.array([3,3])\n",
        "        \n",
        "    \n",
        "    \n",
        "    state, reward, done, info, coords = env.step(env.convert_actions(action))\n",
        "    # print(action)\n",
        "    env.render()\n",
        "    score += reward\n",
        "    if done:\n",
        "        env.close()\n",
        "        print('CAR CRASH')\n",
        "        break\n",
        "    # print(state, score, done, info, coords)\n",
        "    # for wheel in ['rf','lf','rb','lb']:\n",
        "    #   coord = coords[wheel]\n",
        "    #   plt.scatter(coord[0], area_length-coord[1], label=wheel)\n",
        "    # plt.scatter(env.state['pos'][0],area_length-env.state['pos'][1])\n",
        "    # # Add labels and a legend\n",
        "    # plt.xlabel('X-axis')\n",
        "    # plt.ylabel('Y-axis')\n",
        "    # plt.xlim(0,2500)\n",
        "    # plt.ylim(0,2500)\n",
        "    # plt.title('Car Position')\n",
        "end = time.time()\n",
        "print(end-start)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'Dict' object has no attribute 'low'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[150], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mlow\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Dict' object has no attribute 'low'"
          ]
        }
      ],
      "source": [
        "env.observation_space.low"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "374.5"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# np.linalg.norm(env.state['pos'][0]-env.goal_center[0], env.state['pos'][1]-env.goal_center[1])\n",
        "env.state['pos'][1]-env.goal_center[1]\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
